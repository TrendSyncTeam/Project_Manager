{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edba395a-fb95-469b-a14e-4b15420f934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables\n",
    "from helper import load_env\n",
    "load_env()\n",
    "import os\n",
    "import yaml\n",
    "from crewai import Agent, Task, Crew , LLM\n",
    "#GROQ_API_KEY=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\n",
    "# os.environ[\"GROQ_MODEL_NAME\"] = \"llama3-8b-8192\"\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"groq/llama3-8b-8192\",\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64dd99be-b3b7-4922-a1c9-251cb6b4b573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'project_planning_agent': {'role': 'The Ultimate Project Planner\\n', 'goal': 'To meticulously break down the {project_type} project into actionable tasks, ensuring no detail is overlooked, and setting precise timelines that align with the {project_objectives}.\\n', 'backstory': \"As a veteran project manager, you√¢‚Ç¨‚Ñ¢ve led numerous successful projects, particularly in {industry}. Your keen eye for detail and strategic thinking have always ensured that projects are delivered on time and within scope. Now, you're tasked with planning the next groundbreaking {project_type} project.\\n\", 'allow_delegation': False, 'verbose': True}, 'estimation_agent': {'role': 'Expert Estimation Analyst\\n', 'goal': 'Provide highly accurate time, resource, and effort estimations for each task in the {project_type} project to ensure it is delivered efficiently and on budget.\\n', 'backstory': 'You are the go-to expert for project estimation in {industry}. With a wealth of experience and access to vast historical data, you can predict the resources required for any task with remarkable accuracy. Your precision ensures that the {project_type} project remains feasible and avoids unnecessary delays or budget overruns.\\n', 'allow_delegation': False, 'verbose': True}, 'resource_allocation_agent': {'role': 'Resource Allocation Strategist\\n', 'goal': \"Optimize the allocation of tasks for the {project_type} project by balancing team members' skills, availability, and current workload to maximize efficiency and project success.\\n\", 'backstory': 'With a deep understanding of team dynamics and resource management in {industry}, you have a track record of ensuring that the right person is always assigned to the right task. Your strategic thinking ensures that the {project_type} project team is utilized to its full potential without overburdening any individual.\\n', 'allow_delegation': False, 'verbose': True}} {'task_breakdown': {'description': \"Carefully analyze the project_requirements for the {project_type} project and break them down into individual tasks. Define each task's scope in detail, set achievable timelines, and ensure that all dependencies are accounted for:\\n{project_requirements}\\n\\nTeam members:\\n{team_members}\\n\", 'expected_output': 'A comprehensive list of tasks with detailed descriptions, timelines, dependencies, and deliverables. Your final output MUST include a Gantt chart or similar timeline visualization specific to the {project_type} project.\\n'}, 'time_resource_estimation': {'description': 'Thoroughly evaluate each task in the {project_type} project to estimate the time, resources, and effort required. Use historical data, task complexity, and available resources to provide a realistic estimation for each task.\\n', 'expected_output': 'A detailed estimation report outlining the time, resources, and effort required for each task in the {project_type} project. Your final report MUST include a summary of any risks or uncertainties associated with the estimations.\\n'}, 'resource_allocation': {'description': 'Strategically allocate tasks for the {project_type} project to team members based on their skills, availability, and current workload. Ensure that each task is assigned to the most suitable team member and that the workload is evenly distributed.\\n\\nTeam members:\\n{team_members}\\n', 'expected_output': 'A resource allocation chart showing which team members are responsible for each task in the {project_type} project, along with start and end dates. Your final output MUST also include a summary explaining the rationale behind each allocation decision.\\n'}}\n"
     ]
    }
   ],
   "source": [
    "files = {\n",
    "    'agents':'config/agents.yaml',\n",
    "    'tasks':'config/tasks.yaml'\n",
    "}\n",
    "\n",
    "configs = { } \n",
    "for config_type ,file_path in files.items():\n",
    "    with open(file_path , 'r') as file:\n",
    "        configs[config_type] = yaml.safe_load(file)\n",
    "\n",
    "agents_config = configs['agents']\n",
    "tasks_config = configs['tasks']\n",
    "print(agents_config,tasks_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d10b7624-701e-4d4f-b727-c379e93ebb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class TaskEstimate(BaseModel):\n",
    "    task_name: str = Field(..., description=\"Name of the task\")\n",
    "    estimated_time_hours: float = Field(..., description=\"Estimated time to complete the task in hours\")\n",
    "    required_resources: List[str] = Field(..., description=\"List of resources required to complete the task\")\n",
    "\n",
    "class Milestone(BaseModel):\n",
    "    milestone_name: str = Field(..., description=\"Name of the milestone\")\n",
    "    tasks: List[str] = Field(..., description=\"List of task IDs associated with this milestone\")\n",
    "\n",
    "class ProjectPlan(BaseModel):\n",
    "    tasks: List[TaskEstimate] = Field(..., description=\"List of tasks with their estimates\")\n",
    "    milestones: List[Milestone] = Field(..., description=\"List of project milestones\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd844058-a242-4bd7-84aa-696d2ee13d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "project_planning_agent = Agent(\n",
    "  config=agents_config['project_planning_agent'],\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "estimation_agent = Agent(\n",
    "  config=agents_config['estimation_agent'],\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "resource_allocation_agent = Agent(\n",
    "  config=agents_config['resource_allocation_agent'],\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# Creating Tasks\n",
    "task_breakdown = Task(\n",
    "  config=tasks_config['task_breakdown'],\n",
    "  agent=project_planning_agent\n",
    ")\n",
    "\n",
    "time_resource_estimation = Task(\n",
    "  config=tasks_config['time_resource_estimation'],\n",
    "  agent=estimation_agent\n",
    ")\n",
    "\n",
    "resource_allocation = Task(\n",
    "  config=tasks_config['resource_allocation'],\n",
    "  agent=resource_allocation_agent,\n",
    "  output_pydantic=ProjectPlan # This is the structured output we want\n",
    ")\n",
    "\n",
    "# Creating Crew\n",
    "crew = Crew(\n",
    "  agents=[\n",
    "    project_planning_agent,\n",
    "    estimation_agent,\n",
    "    resource_allocation_agent\n",
    "  ],\n",
    "  tasks=[\n",
    "    task_breakdown,\n",
    "    time_resource_estimation,\n",
    "    resource_allocation\n",
    "  ],\n",
    "  verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad254618-abde-4bc1-96df-730cb7d848da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Project Type:** Website\n",
       "\n",
       "**Project Objectives:** Create a website for a small business\n",
       "\n",
       "**Industry:** Technology\n",
       "\n",
       "**Team Members:**\n",
       "\n",
       "- John Doe (Project Manager)\n",
       "- Jane Doe (Software Engineer)\n",
       "- Bob Smith (Designer)\n",
       "- Alice Johnson (QA Engineer)\n",
       "- Tom Brown (QA Engineer)\n",
       "\n",
       "**Project Requirements:**\n",
       "\n",
       "- Create a responsive design that works well on desktop and mobile devices\n",
       "- Implement a modern, visually appealing user interface with a clean look\n",
       "- Develop a user-friendly navigation system with intuitive menu structure\n",
       "- Include an \"About Us\" page highlighting the company's history and values\n",
       "- Design a \"Services\" page showcasing the business's offerings with descriptions\n",
       "- Create a \"Contact Us\" page with a form and integrated map for communication\n",
       "- Implement a blog section for sharing industry news and company updates\n",
       "- Ensure fast loading times and optimize for search engines (SEO)\n",
       "- Integrate social media links and sharing capabilities\n",
       "- Include a testimonials section to showcase customer feedback and build trust\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "project = 'Website'\n",
    "industry = 'Technology'\n",
    "project_objectives = 'Create a website for a small business'\n",
    "team_members = \"\"\"\n",
    "- John Doe (Project Manager)\n",
    "- Jane Doe (Software Engineer)\n",
    "- Bob Smith (Designer)\n",
    "- Alice Johnson (QA Engineer)\n",
    "- Tom Brown (QA Engineer)\n",
    "\"\"\"\n",
    "project_requirements = \"\"\"\n",
    "- Create a responsive design that works well on desktop and mobile devices\n",
    "- Implement a modern, visually appealing user interface with a clean look\n",
    "- Develop a user-friendly navigation system with intuitive menu structure\n",
    "- Include an \"About Us\" page highlighting the company's history and values\n",
    "- Design a \"Services\" page showcasing the business's offerings with descriptions\n",
    "- Create a \"Contact Us\" page with a form and integrated map for communication\n",
    "- Implement a blog section for sharing industry news and company updates\n",
    "- Ensure fast loading times and optimize for search engines (SEO)\n",
    "- Integrate social media links and sharing capabilities\n",
    "- Include a testimonials section to showcase customer feedback and build trust\n",
    "\"\"\"\n",
    "\n",
    "# Format the dictionary as Markdown for a better display in Jupyter Lab\n",
    "formatted_output = f\"\"\"\n",
    "**Project Type:** {project}\n",
    "\n",
    "**Project Objectives:** {project_objectives}\n",
    "\n",
    "**Industry:** {industry}\n",
    "\n",
    "**Team Members:**\n",
    "{team_members}\n",
    "**Project Requirements:**\n",
    "{project_requirements}\n",
    "\"\"\"\n",
    "# Display the formatted output as Markdown\n",
    "display(Markdown(formatted_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "356eb75e-f82b-43e0-b7df-74cb2e002a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:LiteLLM call failed: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=llama3-8b-8192\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Website\n",
      "\u001b[1m\u001b[94m \n",
      "[2025-03-18 19:17:20][üöÄ CREW 'CREW' STARTED, 8F97F287-78C9-4D8E-A3B0-81C550F262C1]: 2025-03-18 19:17:20.240667\u001b[00m\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[94m \n",
      "[2025-03-18 19:17:20][üìã TASK STARTED: CAREFULLY ANALYZE THE PROJECT_REQUIREMENTS FOR THE WEBSITE PROJECT AND BREAK THEM DOWN INTO INDIVIDUAL TASKS. DEFINE EACH TASK'S SCOPE IN DETAIL, SET ACHIEVABLE TIMELINES, AND ENSURE THAT ALL DEPENDENCIES ARE ACCOUNTED FOR:\n",
      "\n",
      "- CREATE A RESPONSIVE DESIGN THAT WORKS WELL ON DESKTOP AND MOBILE DEVICES\n",
      "- IMPLEMENT A MODERN, VISUALLY APPEALING USER INTERFACE WITH A CLEAN LOOK\n",
      "- DEVELOP A USER-FRIENDLY NAVIGATION SYSTEM WITH INTUITIVE MENU STRUCTURE\n",
      "- INCLUDE AN \"ABOUT US\" PAGE HIGHLIGHTING THE COMPANY'S HISTORY AND VALUES\n",
      "- DESIGN A \"SERVICES\" PAGE SHOWCASING THE BUSINESS'S OFFERINGS WITH DESCRIPTIONS\n",
      "- CREATE A \"CONTACT US\" PAGE WITH A FORM AND INTEGRATED MAP FOR COMMUNICATION\n",
      "- IMPLEMENT A BLOG SECTION FOR SHARING INDUSTRY NEWS AND COMPANY UPDATES\n",
      "- ENSURE FAST LOADING TIMES AND OPTIMIZE FOR SEARCH ENGINES (SEO)\n",
      "- INTEGRATE SOCIAL MEDIA LINKS AND SHARING CAPABILITIES\n",
      "- INCLUDE A TESTIMONIALS SECTION TO SHOWCASE CUSTOMER FEEDBACK AND BUILD TRUST\n",
      "\n",
      "\n",
      "TEAM MEMBERS:\n",
      "\n",
      "- JOHN DOE (PROJECT MANAGER)\n",
      "- JANE DOE (SOFTWARE ENGINEER)\n",
      "- BOB SMITH (DESIGNER)\n",
      "- ALICE JOHNSON (QA ENGINEER)\n",
      "- TOM BROWN (QA ENGINEER)\n",
      "\n",
      "]: 2025-03-18 19:17:20.246342\u001b[00m\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[94m \n",
      "[2025-03-18 19:17:20][ü§ñ AGENT 'THE ULTIMATE PROJECT PLANNER\n",
      "' STARTED TASK]: 2025-03-18 19:17:20.247849\u001b[00m\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mThe Ultimate Project Planner\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mCarefully analyze the project_requirements for the Website project and break them down into individual tasks. Define each task's scope in detail, set achievable timelines, and ensure that all dependencies are accounted for:\n",
      "\n",
      "- Create a responsive design that works well on desktop and mobile devices\n",
      "- Implement a modern, visually appealing user interface with a clean look\n",
      "- Develop a user-friendly navigation system with intuitive menu structure\n",
      "- Include an \"About Us\" page highlighting the company's history and values\n",
      "- Design a \"Services\" page showcasing the business's offerings with descriptions\n",
      "- Create a \"Contact Us\" page with a form and integrated map for communication\n",
      "- Implement a blog section for sharing industry news and company updates\n",
      "- Ensure fast loading times and optimize for search engines (SEO)\n",
      "- Integrate social media links and sharing capabilities\n",
      "- Include a testimonials section to showcase customer feedback and build trust\n",
      "\n",
      "\n",
      "Team members:\n",
      "\n",
      "- John Doe (Project Manager)\n",
      "- Jane Doe (Software Engineer)\n",
      "- Bob Smith (Designer)\n",
      "- Alice Johnson (QA Engineer)\n",
      "- Tom Brown (QA Engineer)\n",
      "\n",
      "\u001b[00m\n",
      "\u001b[1m\u001b[94m \n",
      "[2025-03-18 19:17:20][ü§ñ LLM CALL STARTED]: 2025-03-18 19:17:20.247849\u001b[00m\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[94m \n",
      "[2025-03-18 19:17:20][‚ùå LLM CALL FAILED: 'LITELLM.BADREQUESTERROR: LLM PROVIDER NOT PROVIDED. PASS IN THE LLM PROVIDER YOU ARE TRYING TO CALL. YOU PASSED MODEL=LLAMA3-8B-8192\n",
      " PASS MODEL AS E.G. FOR 'HUGGINGFACE' INFERENCE ENDPOINTS PASS IN `COMPLETION(MODEL='HUGGINGFACE/STARCODER',..)` LEARN MORE: HTTPS://DOCS.LITELLM.AI/DOCS/PROVIDERS']: 2025-03-18 19:17:20.250455\u001b[00m\n",
      "\u001b[91m Error during LLM call: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=llama3-8b-8192\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\u001b[00m\n",
      "\u001b[91m An unknown error occurred. Please check the details below.\u001b[00m\n",
      "\u001b[91m Error details: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=llama3-8b-8192\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\u001b[00m\n",
      "\u001b[1m\u001b[94m \n",
      "[2025-03-18 19:17:20][‚ùå TASK FAILED: CAREFULLY ANALYZE THE PROJECT_REQUIREMENTS FOR THE WEBSITE PROJECT AND BREAK THEM DOWN INTO INDIVIDUAL TASKS. DEFINE EACH TASK'S SCOPE IN DETAIL, SET ACHIEVABLE TIMELINES, AND ENSURE THAT ALL DEPENDENCIES ARE ACCOUNTED FOR:\n",
      "\n",
      "- CREATE A RESPONSIVE DESIGN THAT WORKS WELL ON DESKTOP AND MOBILE DEVICES\n",
      "- IMPLEMENT A MODERN, VISUALLY APPEALING USER INTERFACE WITH A CLEAN LOOK\n",
      "- DEVELOP A USER-FRIENDLY NAVIGATION SYSTEM WITH INTUITIVE MENU STRUCTURE\n",
      "- INCLUDE AN \"ABOUT US\" PAGE HIGHLIGHTING THE COMPANY'S HISTORY AND VALUES\n",
      "- DESIGN A \"SERVICES\" PAGE SHOWCASING THE BUSINESS'S OFFERINGS WITH DESCRIPTIONS\n",
      "- CREATE A \"CONTACT US\" PAGE WITH A FORM AND INTEGRATED MAP FOR COMMUNICATION\n",
      "- IMPLEMENT A BLOG SECTION FOR SHARING INDUSTRY NEWS AND COMPANY UPDATES\n",
      "- ENSURE FAST LOADING TIMES AND OPTIMIZE FOR SEARCH ENGINES (SEO)\n",
      "- INTEGRATE SOCIAL MEDIA LINKS AND SHARING CAPABILITIES\n",
      "- INCLUDE A TESTIMONIALS SECTION TO SHOWCASE CUSTOMER FEEDBACK AND BUILD TRUST\n",
      "\n",
      "\n",
      "TEAM MEMBERS:\n",
      "\n",
      "- JOHN DOE (PROJECT MANAGER)\n",
      "- JANE DOE (SOFTWARE ENGINEER)\n",
      "- BOB SMITH (DESIGNER)\n",
      "- ALICE JOHNSON (QA ENGINEER)\n",
      "- TOM BROWN (QA ENGINEER)\n",
      "\n",
      "]: 2025-03-18 19:17:20.250455\u001b[00m\n",
      "\u001b[1m\u001b[94m \n",
      "[2025-03-18 19:17:20][‚ùå CREW 'CREW' FAILED, 8F97F287-78C9-4D8E-A3B0-81C550F262C1]: 2025-03-18 19:17:20.250455\u001b[00m\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=llama3-8b-8192\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      3\u001b[39m inputs = {\n\u001b[32m      4\u001b[39m   \u001b[33m'\u001b[39m\u001b[33mproject_type\u001b[39m\u001b[33m'\u001b[39m: project,\n\u001b[32m      5\u001b[39m   \u001b[33m'\u001b[39m\u001b[33mproject_objectives\u001b[39m\u001b[33m'\u001b[39m: project_objectives,\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m   \u001b[33m'\u001b[39m\u001b[33mproject_requirements\u001b[39m\u001b[33m'\u001b[39m: project_requirements\n\u001b[32m      9\u001b[39m }\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Run the crew\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m result = \u001b[43mcrew\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkickoff\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m  \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\crewai\\crew.py:619\u001b[39m, in \u001b[36mCrew.kickoff\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m metrics: List[UsageMetrics] = []\n\u001b[32m    618\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process == Process.sequential:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_sequential_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process == Process.hierarchical:\n\u001b[32m    621\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._run_hierarchical_process()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\crewai\\crew.py:731\u001b[39m, in \u001b[36mCrew._run_sequential_process\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_sequential_process\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> CrewOutput:\n\u001b[32m    730\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Executes tasks sequentially and returns the final output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m731\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_tasks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\crewai\\crew.py:829\u001b[39m, in \u001b[36mCrew._execute_tasks\u001b[39m\u001b[34m(self, tasks, start_index, was_replayed)\u001b[39m\n\u001b[32m    826\u001b[39m     futures.clear()\n\u001b[32m    828\u001b[39m context = \u001b[38;5;28mself\u001b[39m._get_context(task, task_outputs)\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m task_output = \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_sync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[43m=\u001b[49m\u001b[43magent_to_use\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools_for_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    834\u001b[39m task_outputs.append(task_output)\n\u001b[32m    835\u001b[39m \u001b[38;5;28mself\u001b[39m._process_task_result(task, task_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\crewai\\task.py:304\u001b[39m, in \u001b[36mTask.execute_sync\u001b[39m\u001b[34m(self, agent, context, tools)\u001b[39m\n\u001b[32m    297\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute_sync\u001b[39m(\n\u001b[32m    298\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    299\u001b[39m     agent: Optional[BaseAgent] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    300\u001b[39m     context: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    301\u001b[39m     tools: Optional[List[BaseTool]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    302\u001b[39m ) -> TaskOutput:\n\u001b[32m    303\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Execute the task synchronously.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\crewai\\task.py:448\u001b[39m, in \u001b[36mTask._execute_core\u001b[39m\u001b[34m(self, agent, context, tools)\u001b[39m\n\u001b[32m    446\u001b[39m \u001b[38;5;28mself\u001b[39m.end_time = datetime.datetime.now()\n\u001b[32m    447\u001b[39m crewai_event_bus.emit(\u001b[38;5;28mself\u001b[39m, TaskFailedEvent(error=\u001b[38;5;28mstr\u001b[39m(e)))\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\crewai\\task.py:368\u001b[39m, in \u001b[36mTask._execute_core\u001b[39m\u001b[34m(self, agent, context, tools)\u001b[39m\n\u001b[32m    366\u001b[39m \u001b[38;5;28mself\u001b[39m.processed_by_agents.add(agent.role)\n\u001b[32m    367\u001b[39m crewai_event_bus.emit(\u001b[38;5;28mself\u001b[39m, TaskStartedEvent(context=context))\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m result = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_task\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    374\u001b[39m pydantic_output, json_output = \u001b[38;5;28mself\u001b[39m._export_output(result)\n\u001b[32m    375\u001b[39m task_output = TaskOutput(\n\u001b[32m    376\u001b[39m     name=\u001b[38;5;28mself\u001b[39m.name,\n\u001b[32m    377\u001b[39m     description=\u001b[38;5;28mself\u001b[39m.description,\n\u001b[32m   (...)\u001b[39m\u001b[32m    383\u001b[39m     output_format=\u001b[38;5;28mself\u001b[39m._get_output_format(),\n\u001b[32m    384\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\crewai\\agent.py:265\u001b[39m, in \u001b[36mAgent.execute_task\u001b[39m\u001b[34m(self, task, context, tools)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m e.\u001b[34m__class__\u001b[39m.\u001b[34m__module__\u001b[39m.startswith(\u001b[33m\"\u001b[39m\u001b[33mlitellm\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    256\u001b[39m     \u001b[38;5;66;03m# Do not retry on litellm errors\u001b[39;00m\n\u001b[32m    257\u001b[39m     crewai_event_bus.emit(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         event=AgentExecutionErrorEvent(\n\u001b[32m   (...)\u001b[39m\u001b[32m    263\u001b[39m         ),\n\u001b[32m    264\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    266\u001b[39m \u001b[38;5;28mself\u001b[39m._times_executed += \u001b[32m1\u001b[39m\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._times_executed > \u001b[38;5;28mself\u001b[39m.max_retry_limit:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\crewai\\agent.py:246\u001b[39m, in \u001b[36mAgent.execute_task\u001b[39m\u001b[34m(self, task, context, tools)\u001b[39m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    237\u001b[39m     crewai_event_bus.emit(\n\u001b[32m    238\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    239\u001b[39m         event=AgentExecutionStartedEvent(\n\u001b[32m   (...)\u001b[39m\u001b[32m    244\u001b[39m         ),\n\u001b[32m    245\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magent_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_names\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magent_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtools_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magent_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtools_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mask_for_human_input\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhuman_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m e.\u001b[34m__class__\u001b[39m.\u001b[34m__module__\u001b[39m.startswith(\u001b[33m\"\u001b[39m\u001b[33mlitellm\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    256\u001b[39m         \u001b[38;5;66;03m# Do not retry on litellm errors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:119\u001b[39m, in \u001b[36mCrewAgentExecutor.invoke\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._handle_unknown_error(e)\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m e.\u001b[34m__class__\u001b[39m.\u001b[34m__module__\u001b[39m.startswith(\u001b[33m\"\u001b[39m\u001b[33mlitellm\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    118\u001b[39m     \u001b[38;5;66;03m# Do not retry on litellm errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:108\u001b[39m, in \u001b[36mCrewAgentExecutor.invoke\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28mself\u001b[39m.ask_for_human_input = \u001b[38;5;28mbool\u001b[39m(inputs.get(\u001b[33m\"\u001b[39m\u001b[33mask_for_human_input\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     formatted_answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m:\n\u001b[32m    110\u001b[39m     \u001b[38;5;28mself\u001b[39m._printer.print(\n\u001b[32m    111\u001b[39m         content=\u001b[33m\"\u001b[39m\u001b[33mAgent failed to reach a final answer. This is likely a bug - please report it.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    112\u001b[39m         color=\u001b[33m\"\u001b[39m\u001b[33mred\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    113\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:166\u001b[39m, in \u001b[36mCrewAgentExecutor._invoke_loop\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    164\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m e.\u001b[34m__class__\u001b[39m.\u001b[34m__module__\u001b[39m.startswith(\u001b[33m\"\u001b[39m\u001b[33mlitellm\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    165\u001b[39m         \u001b[38;5;66;03m# Do not retry on litellm errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_context_length_exceeded(e):\n\u001b[32m    168\u001b[39m         \u001b[38;5;28mself\u001b[39m._handle_context_length()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:146\u001b[39m, in \u001b[36mCrewAgentExecutor._invoke_loop\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    142\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[38;5;28mself\u001b[39m._enforce_rpm_limit()\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_llm_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m formatted_answer = \u001b[38;5;28mself\u001b[39m._process_llm_response(answer)\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(formatted_answer, AgentAction):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:216\u001b[39m, in \u001b[36mCrewAgentExecutor._get_llm_response\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    212\u001b[39m     \u001b[38;5;28mself\u001b[39m._printer.print(\n\u001b[32m    213\u001b[39m         content=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError during LLM call: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    214\u001b[39m         color=\u001b[33m\"\u001b[39m\u001b[33mred\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    215\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m answer:\n\u001b[32m    219\u001b[39m     \u001b[38;5;28mself\u001b[39m._printer.print(\n\u001b[32m    220\u001b[39m         content=\u001b[33m\"\u001b[39m\u001b[33mReceived None or empty response from LLM call.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    221\u001b[39m         color=\u001b[33m\"\u001b[39m\u001b[33mred\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    222\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:207\u001b[39m, in \u001b[36mCrewAgentExecutor._get_llm_response\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Call the LLM and return the response, handling any invalid responses.\"\"\"\u001b[39;00m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m     answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    212\u001b[39m     \u001b[38;5;28mself\u001b[39m._printer.print(\n\u001b[32m    213\u001b[39m         content=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError during LLM call: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    214\u001b[39m         color=\u001b[33m\"\u001b[39m\u001b[33mred\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    215\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\crewai\\llm.py:310\u001b[39m, in \u001b[36mLLM.call\u001b[39m\u001b[34m(self, messages, tools, callbacks, available_functions)\u001b[39m\n\u001b[32m    307\u001b[39m params = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params.items() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[32m    309\u001b[39m \u001b[38;5;66;03m# --- 2) Make the completion call\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m response = \u001b[43mlitellm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m response_message = cast(Choices, cast(ModelResponse, response).choices)[\n\u001b[32m    312\u001b[39m     \u001b[32m0\u001b[39m\n\u001b[32m    313\u001b[39m ].message\n\u001b[32m    314\u001b[39m text_response = response_message.content \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\litellm\\utils.py:1154\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[32m   1151\u001b[39m     logging_obj.failure_handler(\n\u001b[32m   1152\u001b[39m         e, traceback_exception, start_time, end_time\n\u001b[32m   1153\u001b[39m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1154\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\litellm\\utils.py:1032\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1030\u001b[39m         print_verbose(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1031\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1032\u001b[39m result = \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1033\u001b[39m end_time = datetime.datetime.now()\n\u001b[32m   1034\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\litellm\\main.py:3068\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[39m\n\u001b[32m   3065\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m   3066\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   3067\u001b[39m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3068\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_type(\n\u001b[32m   3069\u001b[39m         model=model,\n\u001b[32m   3070\u001b[39m         custom_llm_provider=custom_llm_provider,\n\u001b[32m   3071\u001b[39m         original_exception=e,\n\u001b[32m   3072\u001b[39m         completion_kwargs=args,\n\u001b[32m   3073\u001b[39m         extra_kwargs=kwargs,\n\u001b[32m   3074\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\litellm\\main.py:979\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[39m\n\u001b[32m    977\u001b[39m     model = deployment_id\n\u001b[32m    978\u001b[39m     custom_llm_provider = \u001b[33m\"\u001b[39m\u001b[33mazure\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m model, custom_llm_provider, dynamic_api_key, api_base = \u001b[43mget_llm_provider\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    987\u001b[39m     provider_specific_header \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    988\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m provider_specific_header[\u001b[33m\"\u001b[39m\u001b[33mcustom_llm_provider\u001b[39m\u001b[33m\"\u001b[39m] == custom_llm_provider\n\u001b[32m    989\u001b[39m ):\n\u001b[32m    990\u001b[39m     headers.update(provider_specific_header[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\litellm\\litellm_core_utils\\get_llm_provider_logic.py:356\u001b[39m, in \u001b[36mget_llm_provider\u001b[39m\u001b[34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    355\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, litellm.exceptions.BadRequestError):\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m         error_str = (\n\u001b[32m    359\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGetLLMProvider Exception - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33moriginal model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    360\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\litellm\\litellm_core_utils\\get_llm_provider_logic.py:333\u001b[39m, in \u001b[36mget_llm_provider\u001b[39m\u001b[34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[39m\n\u001b[32m    331\u001b[39m     error_str = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m Pass model as E.g. For \u001b[39m\u001b[33m'\u001b[39m\u001b[33mHuggingface\u001b[39m\u001b[33m'\u001b[39m\u001b[33m inference endpoints pass in `completion(model=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mhuggingface/starcoder\u001b[39m\u001b[33m'\u001b[39m\u001b[33m,..)` Learn more: https://docs.litellm.ai/docs/providers\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    332\u001b[39m     \u001b[38;5;66;03m# maps to openai.NotFoundError, this is raised when openai does not recognize the llm\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m litellm.exceptions.BadRequestError(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    334\u001b[39m         message=error_str,\n\u001b[32m    335\u001b[39m         model=model,\n\u001b[32m    336\u001b[39m         response=httpx.Response(\n\u001b[32m    337\u001b[39m             status_code=\u001b[32m400\u001b[39m,\n\u001b[32m    338\u001b[39m             content=error_str,\n\u001b[32m    339\u001b[39m             request=httpx.Request(method=\u001b[33m\"\u001b[39m\u001b[33mcompletion\u001b[39m\u001b[33m\"\u001b[39m, url=\u001b[33m\"\u001b[39m\u001b[33mhttps://github.com/BerriAI/litellm\u001b[39m\u001b[33m\"\u001b[39m),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    340\u001b[39m         ),\n\u001b[32m    341\u001b[39m         llm_provider=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    342\u001b[39m     )\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_base \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(api_base, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    344\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[32m    345\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mapi base needs to be a string. api_base=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(api_base)\n\u001b[32m    346\u001b[39m     )\n",
      "\u001b[31mBadRequestError\u001b[39m: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=llama3-8b-8192\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers"
     ]
    }
   ],
   "source": [
    "# The given Python dictionary\n",
    "print(project)\n",
    "inputs = {\n",
    "  'project_type': project,\n",
    "  'project_objectives': project_objectives,\n",
    "  'industry': industry,\n",
    "  'team_members': team_members,\n",
    "  'project_requirements': project_requirements\n",
    "}\n",
    "\n",
    "# Run the crew\n",
    "result = crew.kickoff(\n",
    "  inputs=inputs\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e51a8d-a7ad-48d1-a574-cc4d548908d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bbf34e-9bef-481a-81d1-ba167cf7b52a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a19b78e-1ad2-4669-827c-c6e6bb46d887",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
